[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Computational Physics",
    "section": "",
    "text": "Welcome to the course material for Computational Physics.\nThe course covers 3 broad topics :\n\nLinear Algebra (Sections 2-4)\nPartial Differential Equations (Section 5 & 6)\nMonte Carlo Methods (Sections 7 & 8)\n\nEach section describes a class of numerical methods, and will cover both the mathematical background, and use of relevant scipy routines with Python code examples. In addition there are several fully worked examples of solving physics problems. You may wish to copy the code examples to a Jupyter notebook to fully explore them.\nThis book was built with Quarto https://quarto.org/.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "linalg-1.html",
    "href": "linalg-1.html",
    "title": "2  Basic Matrix Operations",
    "section": "",
    "text": "Python/numpy/scipy provide a range of options for achieving basic matrix operations. You will need to take a little care to ensure that your code implements the operations you intend it to. This is largely due to the fact that some operators/functions will change their behaviour depending on the input you provide. This section includes some recommendations for simple linear algebra, which should ensure your code behaves as desired.\nMatrices can be implemented as a 2D np.ndarray. Basic matrix arithmetic can then be performed using standard operators +,- and @. You can also use np.matmul() for matrix multiplication. Numpy will also perform matrix multiplication with np.dot(), but this is not recommended if you can use @ or np.matmul().\n\nimport numpy as np\n\nA = np.array([[1, 2, 3,],[4, 5, 6],[1, 0, 0]])\nB = np.array([[1, 0, 0,],[0, 1, 0],[0, 0, 1]])\n\n# addition\nprint(A + B)\n\n# subtraction\nprint(A - B)\n\n# scalar multiplication\nprint(3*B)\n\n# matrix multiplication\nprint(np.matmul(A, B))\nprint(A @ B)\n\n[[2 2 3]\n [4 6 6]\n [1 0 1]]\n[[ 0  2  3]\n [ 4  4  6]\n [ 1  0 -1]]\n[[3 0 0]\n [0 3 0]\n [0 0 3]]\n[[1 2 3]\n [4 5 6]\n [1 0 0]]\n[[1 2 3]\n [4 5 6]\n [1 0 0]]\n\n\nVectors can be implemented as 1D arrays, or as 2D arrays. A 1D array will be interpreted as row or column vector depending on the context in which it is used. Use of 2D arrays allows you to specify row or column form. This can be useful, since np.matmul() or @ will throw an exception if you accidentally try to perform an illegal matrix operation.\n\nv  = np.array([1,2,3])\nvr = np.array([[1,2,3]])\nvc = np.array([[1],[2],[3]])\n\n# two options for matrix * vector\nprint(A@v)\nprint(A@vc)\n\n# two options for vector * matrix\nprint(v@A)\nprint(vr@A)\n\n# this is not a valid matrix multiplication !\nprint(A@vr)\n\n[14 32  1]\n[[14]\n [32]\n [ 1]]\n[12 12 15]\n[[12 12 15]]\n\n\nValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)-&gt;(n?,m?) (size 1 is different from 3)\n\n\nNumpy will also provide the usual forms of vector product via np.vdot(), np.cross(), np.inner() and np.outer(). Again, np.dot() will provide a vector dot product, but is not recommended if you can use vdot().\nOther useful matrix operations are provided by numpy, such as : - np.transpose() (also available via ndarray.T) - np.norm() - np.trace()\nFor further information, look at the reference pages : https://numpy.org/doc/stable/reference/routines.array-manipulation.html https://numpy.org/doc/stable/reference/routines.linalg.html\nFinally, scipy.linalg provides some additional basic operations such as the determinant and the inverse.\n\nimport numpy as np\nimport scipy.linalg as linalg\n\nA = np.array([[1, 2, 3,],[4, 5, 6],[1, 0, 0]])\ndetA = linalg.det(A)\nprint(detA)\n\ninvA = linalg.inv(A)\nprint(invA)\n\n-3.000000000000001\n[[-0.          0.          1.        ]\n [-2.          1.         -2.        ]\n [ 1.66666667 -0.66666667  1.        ]]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Basic Matrix Operations</span>"
    ]
  },
  {
    "objectID": "linalg-2.html",
    "href": "linalg-2.html",
    "title": "3  Simultaneous Equations",
    "section": "",
    "text": "3.1 Inverse Matrix\nOne way to solve the above equation is to multiply both sides by the inverse of A:\n\\[A^{-1} A x = A^{-1} y \\tag{3.4}\\]\ngiving :\n\\[x = A^{-1} y \\tag{3.5}\\]\nThis is demonstrated in the example below for a simple test case :\n\\[\n\\left(\\begin{array}{ccc} 1 & 2 & 2 \\\\\n                        3 & 1 & 6 \\\\\n                        0 & 2 & 2\\end{array}\\right)\n\\left(\\begin{array}{c} x_1 \\\\ x_2 \\\\ x_3\\end{array}\\right) =\n\\left(\\begin{array}{c} 2 \\\\ 7 \\\\ 1\\end{array}\\right)\n\\tag{3.6}\\]\nimport numpy as np\nimport scipy.linalg as linalg\n\ndef solve_inv(a,y):\n    x = linalg.inv(a) @ y\n    return x\n\na = np.array([[1, 2, 2,],[3, 1, 6],[0, 2, 2]])\ny = np.array([[2], [7], [1]])\nprint(a)\nprint(y)\n\nx = solve_inv(a,y)\nprint(x)\n\n[[1 2 2]\n [3 1 6]\n [0 2 2]]\n[[2]\n [7]\n [1]]\n[[ 1. ]\n [-0.2]\n [ 0.7]]\nIs this the solution? We can easily check by inserting the solution into the original equation.\nprint(a @ x)\n\n[[2.]\n [7.]\n [1.]]\nWhich is indeed equal to our y above. This kind of test is known as a ‘closure test’ and will be used frequently throughout this unit to verify our code.\nBefore using this method for solving simultaneous equations, though, we should understand how scipy.linalg.inv finds the matrix inverse. Unfortunately, this is tricky to understand from the reference page, (scipy.linalg.inv). But, if you examine the source code for this function you’ll see that it uses a function called DGETRI. This is defined in the LAPACK library, and its reference page is here. As you can see this routine uses LU decomposition to find the inverse! It doesn’t make sense, therefore, to find a matrix inverse simply to solve a simultaneous equation, and using LU decomposition directly will involve fewer operations. However, there are exceptions when dealing with many simultaneous equations. For example, suppose you have a sequence of problems which all feature the same matrix \\(A\\), but have different RHS \\(y\\). In this case it would be efficient to invert \\(A\\) once, then multiply by \\(y\\) to solve each problem, since multiplication involves fewer operations than LU decomposition.\nAs an aside, LAPACK is a linear algebra library written in FORTRAN - which remains one of the most efficient languages for writing numerical methods - and most routines in scipy.linalg basically provide a Python interface to this library.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Simultaneous Equations</span>"
    ]
  },
  {
    "objectID": "linalg-2.html#gaussian-elimination",
    "href": "linalg-2.html#gaussian-elimination",
    "title": "3  Simultaneous Equations",
    "section": "3.2 Gaussian Elimination",
    "text": "3.2 Gaussian Elimination\nSome sets of simultaneous equations are easy to solve. For example :\n\\[\n\\begin{aligned}\na x_1 &= y_1 \\\\\nb x_2 &= y_2 \\\\\nc x_3 &= y_3\n\\end{aligned}\n\\tag{3.7}\\] This can be written in what is known as row echelon form :\n\\[\n\\left(\\begin{array}{ccc} a & 0 & 0 \\\\\n                        0 & b & 0 \\\\\n                        0 & 0 & c\\end{array}\\right)\n\\left(\\begin{array}{c} x_1 \\\\ x_2 \\\\ x_3\\end{array}\\right) =\n\\left(\\begin{array}{c} y_1 \\\\ y_2 \\\\ y_3\\end{array}\\right)\n\\tag{3.8}\\]\nAnd then reduced row echelon form :\n\\[\n\\left(\\begin{array}{ccc} 1 & 0 & 0 \\\\\n                        0 & 1 & 0 \\\\\n                        0 & 0 & 1\\end{array}\\right)\n\\left(\\begin{array}{c} x_1 \\\\ x_2 \\\\ x_3\\end{array}\\right) =\n\\left(\\begin{array}{c} y_1/a \\\\ y_2/b \\\\ y_3/c\\end{array}\\right)\n\\tag{3.9}\\]\nGauss-Jordan elimination is a process which reduces any linear equation set to this form. It can be shown that the reduced row echelon form is unique, and therefore independent of the order of operations which are used to find it. The technique is illustrated using the example problem from earlier. It’s convenient to use the ‘augmented’ matrix, which includes the right-hand side. \\[\n\\left(\\begin{array}{ccc|c}\n    1 & 2 & 2 & 2 \\\\\n    3 & 1 & 6 & 7 \\\\\n    0 & 2 & 2 & 1\n\\end{array}\\right)\n\\tag{3.10}\\]\nThen we apply simple operations until we obtain the equation in row echelon form. These operations include:\n\nMultiply a row by a constant\nSwap two rows\nSum two rows in a linear combination\n\n(Hopefully, these sound reasonably familiar - we are just formalising techniques you will have used before)\nReplace \\(R_1\\) (row 1) with \\(R_1 - R_3\\) : \\[\n\\left(\\begin{array}{ccc|c}\n    1 & 0 & 0 & 1 \\\\\n    3 & 1 & 6 & 7 \\\\\n    0 & 2 & 2 & 1\n\\end{array}\\right)\n\\tag{3.11}\\]\nReplace \\(R_2\\) with \\(R_2 - 3R_1\\) : \\[\n\\left(\\begin{array}{ccc|c}\n    1 & 0 & 0 & 1 \\\\\n    0 & 1 & 6 & 4 \\\\\n    0 & 2 & 2 & 1\n\\end{array}\\right)\n\\tag{3.12}\\]\nReplace \\(R_2\\) with \\(R_2 - \\frac{2}{5}R_2\\) : \\[\n\\left(\\begin{array}{ccc|c}\n    1 & 0 & 0 & 1 \\\\\n    0 & -5 & 0 & 1 \\\\\n    0 & 0 & 2 & \\frac{7}{5}\n\\end{array}\\right)\n\\tag{3.13}\\]\nAnd then finally for reduced row echelon form, replace \\(R_2\\) with \\(\\frac{-1}{5}R_2\\) and \\(R_3\\) with \\(\\frac{1}{2}R_3\\) \\[\n\\left(\\begin{array}{ccc|c}\n    1 & 0 & 0 & 1 \\\\\n    0 & 1 & 0 & -\\frac{1}{5} \\\\\n    0 & 0 & 1 & \\frac{7}{10}\n\\end{array}\\right)\n\\tag{3.14}\\]\nSo the solution is : \\[\n\\begin{aligned}\nx_1 &= 1 \\\\\nx_2 &= -\\frac{1}{5} \\\\\nx_3 &= \\frac{7}{10}\n\\end{aligned}\n\\tag{3.15}\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Simultaneous Equations</span>"
    ]
  },
  {
    "objectID": "linalg-2.html#lu-decomposition",
    "href": "linalg-2.html#lu-decomposition",
    "title": "3  Simultaneous Equations",
    "section": "3.3 LU Decomposition",
    "text": "3.3 LU Decomposition\nMatrix decomposition techniques involve factorising a general matrix into a product of several matrices. LU decomposition involves writing the general matrix, \\(A\\), as the product of two triangular matrices, \\(L\\) and \\(U\\).\n\\[\nA=\n\\left(\\begin{array}{ccc} a_{11} & a_{12} & a_{13} \\\\\n                         a_{21} & a_{22} & a_{23} \\\\\n                         a_{31} & a_{32} & a_{33}\n\\end{array}\\right)\n\\tag{3.16}\\]\n\\[\nA=LU=\n\\left(\\begin{array}{ccc} 1      & 0      & 0 \\\\\n                         l_{21} & 1      & 0 \\\\\n                         l_{31} & l_{32} & 1\n\\end{array}\\right)\n\\left(\\begin{array}{ccc} u_{11} & u_{12} & u_{13} \\\\\n                         0      & u_{22} & u_{23} \\\\\n                         0      & 0      & u_{33}\n\\end{array}\\right)\n\\tag{3.17}\\]\nWe can use LU decomposition to solve matrix equations since it allows us to write the equation \\[Ax = y\\] as \\(L(Ux)=y\\). This can then be written as two equations \\(Lc=y\\) and \\(Ux=c\\), which are trivially solved, first for \\(c\\), and then for \\(x\\).\nThe matrices \\(L\\) and \\(U\\) can be found using the operations described above for Gaussian elimination. There are several algorithmic formulations that define the sequence of operations. Scipy provides an LU decomposition routine: scipy.linalg.lu(). Note that this performs a variation on the LU decomposition described above, since it also computes a permutation matrix \\(P\\), such that\n\\[PA = LU \\tag{3.18}\\]\nScipy also provides a simple function to obtain the solutions to a matrix equation. scipy.linalg.lu_solve() expects the \\(L\\), \\(U\\) and \\(P\\) matrices as arguments, as shown in the example below.\n\nscipy.linalg.lu\nscipy.linalg.lu_solve\n\n\ndef solve_lu(a,y):\n    lu, piv = linalg.lu_factor(a)\n    x = linalg.lu_solve((lu, piv), y)\n    return x\n\nprint(solve_lu(a,y))\n\n[[ 1. ]\n [-0.2]\n [ 0.7]]\n\n\nNote that the general purpose solvers provided by both numpy and scipy both utilise LU decomposition :\n\nnumpy.linalg.solve\nscipy.linalg.solve",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Simultaneous Equations</span>"
    ]
  },
  {
    "objectID": "linalg-2.html#svd-decomposition",
    "href": "linalg-2.html#svd-decomposition",
    "title": "3  Simultaneous Equations",
    "section": "3.4 SVD Decomposition",
    "text": "3.4 SVD Decomposition\nLU decomposition will find an exact solution to the matrix equation in a wide variety of cases. However, a solution may not exist, or there may be infinite solutions. In such cases, the Singular Value Decomposition may be of use.\nFor an \\(m \\times n\\) matrix \\(A\\), the singular values, \\(\\sigma\\) are given by the solutions to\n\\[\n\\begin{aligned}\nAv &= \\sigma u \\\\\nA^Tu &=\\sigma v\n\\end{aligned}\n\\tag{3.19}\\]\nwhere \\(u\\) and \\(v\\) are two non-zero vectors. These equations are closely related to the eigenvalue equation. Indeed, the singular values are also the square roots of the eigenvalues of \\(A^TA\\).\nThe singular value decomposition of \\(A\\) is\n\\[A = U\\Sigma V^T \\tag{3.20}\\]\nwhere \\(U\\) and \\(V\\) are orthonormal matrices, and \\(\\Sigma\\) is a matrix with the singular values on its leading diagonal, and zero elsewhere.\nThe SVD decomposition allows use to compute the pseudo-inverse of \\(A\\), which is given by :\n\\[A^\\dagger = V \\Sigma^\\dagger U^T \\tag{3.21}\\]\nwhere \\(\\Sigma^\\dagger\\) is the pseudo-inverse of \\(\\Sigma\\) and is obtained by transposing \\(\\Sigma\\) and replacing each non-zero element with it’s reciprocal.\nThe pseudoinverse (also known as the Moore-Penrose inverse) can always be computed, even when \\(A\\) is singular, ie. when when \\(\\frac{1}{|A|}=0\\) and the inverse cannot be found.\nIn the context of solving a matrix equation \\(Ax=y\\), the product of pseudoinverse and the RHS, (i.e. \\(\\bar{X}=A^\\dagger y\\)) has various properties. When A is non-singular, \\(\\bar{x}\\) gives the solution to \\(Ax=y\\). When \\(A\\) is singular, \\(\\bar{x}\\) is a least squares approximation to the nearest solution. When \\(Ax=y\\) has a space of solutions (equivalent to a set of simultaneous equations with degeneracy), then \\(\\bar{x}\\) is a vector which describes this space.\nSVD decomposition is available in Scipy using scipy.linalg.svd(). For further information, see scipy.linalg.svd. Note that, unlike LU decomposition, no solve() function is supplied, and instead we must write some code to calculate \\(\\bar{x}\\).\n\ndef solve_svd(a,y):\n    u, s, v = linalg.svd(a)\n    x = v.T @ np.diag(1/s) @ u.T @ y\n    return x\n\nprint(solve_svd(a,y))\n\n[[ 1. ]\n [-0.2]\n [ 0.7]]\n\n\nAlternatively, a function to compute the pseudoinverse directly is provided, scipy.linalg.pinv.\n\nprint( linalg.pinv(a) @ y)\n\n[[ 1. ]\n [-0.2]\n [ 0.7]]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Simultaneous Equations</span>"
    ]
  },
  {
    "objectID": "linalg-2.html#physics-example",
    "href": "linalg-2.html#physics-example",
    "title": "3  Simultaneous Equations",
    "section": "3.5 Physics Example",
    "text": "3.5 Physics Example\nHere we illustrate the use of simultaneous equation solvers in a familiar context - the use of Kirchoff’s laws and Ohm’s law to understand resistor networks. Typically, analysis of a resistor network will involve solving simultaneous equations, to calculate voltage and current at the desired points in the network. Consider the electronic circuit shown in the diagram.\n\n\n\nFigure 1 - Example resistor network.\n\n\nWhere : \\(V_1 = 12V\\), \\(V_2 = 12V\\), \\(R_1 = 3 \\Omega\\), \\(R_2 = 3 \\Omega\\), \\(R_3 = 10 \\Omega\\), \\(R_4 = 2 \\Omega\\), \\(R_5 = 2 \\Omega\\).\nBy identifying the three current loops indicated, we can use Kirchoff’s loop rule and Ohm’s law to write : \\[\n\\begin{aligned}\nV_1 &= I_1 R_1 + (I_1 - I_2) R_2 \\\\\n0   &= (I_2 - I_1) R_2 + I_2 R_3 + (I_2 - I_3) R_4 \\\\\n-V_2 &= (I_3 - I_2) R_4 + I_3 R_5\n\\end{aligned}\n\\tag{3.22}\\]\nSo we have a set of simultaneous equations, which we can write as a matrix equation :\n\\[\n\\begin{pmatrix}\nR_1+R_2 & -R_2 & 0 \\\\\n-R_2 & R_2+R_3+R_4 & -R_4 \\\\\n0  & -R_4 & R_4+R_5 \\\\\n\\end{pmatrix}\n\\begin{pmatrix}\nI_1 \\\\\nI_2 \\\\\nI_3\n\\end{pmatrix}\n=\n\\begin{pmatrix}\nV_1 \\\\\n0 \\\\\n-V_2\n\\end{pmatrix}\n\\tag{3.23}\\]\nSolving this matrix equation will provide the current at all points in the circuit. This method is known as “mesh analysis” of circuits.\nWe can write a simple function that, given the voltage and resistor values, will return the currents :\n\ndef meshAnalysis(v1, v2, r1, r2, r3, r4, r5):\n    m = np.array ([[r1+r2, -r2, 0],[-r2,r2+r3+r4,-r4],[0,-r4,r4+r5]])\n    v = np.array([[v1],[0],[-v2]])\n    i = linalg.solve(m,v)\n    return i\n\nWhich, for the values given, will return the three currents :\n\ni = meshAnalysis(12, 12, 3, 3, 10, 2, 2)\n\nprint(i)\n\n[[ 2.]\n [ 0.]\n [-3.]]\n\n\nOr we could calculate, for example, how \\(I_2\\) will vary as a function of \\(R_4\\), with all other values fixed :\n\nr4s = np.linspace(0.1, 5.0, 100)\ni2s = np.empty(len(r4s))\n\nfor j,r4 in enumerate(r4s):\n    i     = meshAnalysis(12, 12, 3, 3, 10, r4, 2)\n    i2s[j] = i[1][0]\n\nimport matplotlib.pyplot as plt\n\nplt.plot(r4s, i2s)\nplt.ylabel('$I_2$ (A)')\nplt.xlabel('$R_4$ (ohm)')\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Simultaneous Equations</span>"
    ]
  },
  {
    "objectID": "linalg-3.html",
    "href": "linalg-3.html",
    "title": "4  Eigenproblems",
    "section": "",
    "text": "4.1 Simple example\nWe can test these sovlers using the matrix :\n\\[A =\n\\pmatrix{\n-2 & -4 & 2 \\\\\n-2 &  1 & 2 \\\\\n4  &  2 & 5}\n\\tag{4.3}\\]\nfor which the eigenvalues are \\(\\lambda^{(0)}=6\\), \\(\\lambda^{(1)}=-5\\), \\(\\lambda^{(2)}=3\\).\nNote that the algorithms discussed here will all find unit eigenvectors. The eigenvector corresponding to \\(\\lambda^{(0)}\\) is then :\n\\[\\hat{u}^{(0)}=\\pmatrix{\\frac{1}{\\sqrt{293}} \\\\\n\\frac{6}{\\sqrt{293}} \\\\\n\\frac{16}{\\sqrt{293}}\n}\n=\n\\pmatrix{0.058 \\\\\n0.351 \\\\\n0.935} \\tag{4.4}\\]\nwith numerical values given to 3 decimal places on the RHS.\nimport numpy as np\nimport scipy.linalg as linalg\n\nm = np.array([[-2,-4,2],[-2,1,2],[4,2,5]])\n\n# set seed for repeatability\nnp.random.seed(2)\n\n# run the algorithm\nmus, vs = linalg.eig(m)\n\n# print results\nnp.set_printoptions(precision=3)\nfor i in range(3):\n    print(\"Eigenvalue/vector : {:.1f} {}\".format(mus[i], vs.T[i]))\n\nEigenvalue/vector : -5.0+0.0j [ 0.816  0.408 -0.408]\nEigenvalue/vector : 3.0+0.0j [ 0.535 -0.802 -0.267]\nEigenvalue/vector : 6.0+0.0j [0.058 0.351 0.935]\nWhich includes the solution expected.\nNote that we have transposed the array of eigenvectors returned by linalg.eig(). This is a feature of the function, as described in the reference manual : “The normalized left eigenvector corresponding to the eigenvalue w[i] is the column vl[:,i]”.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Eigenproblems</span>"
    ]
  },
  {
    "objectID": "linalg-3.html#physics-example",
    "href": "linalg-3.html#physics-example",
    "title": "4  Eigenproblems",
    "section": "4.2 Physics Example",
    "text": "4.2 Physics Example\nIn this section, we illustrate the use of eigenvalue solvers in finding stable solutions of the coupled system of oscillators shown below.\n\n\n\nFigure 2 - Coupled oscillators, masses on springs.\n\n\nIf the displacement of the \\(i\\)th mass from its equilibrium position is denoted as \\(x_i\\), the force on the mass is given by the tension in the two springs as :\n\\[F_i = −k(x_i − x_{i−1}) + k(x_{i+1} − x_i) = −k(2x_i − x_{i−1} − x_{i+1}) \\tag{4.5}\\]\nWe can assume that there are normal mode solutions, i.e. solutions of the form \\(x_i = z_i e^{i\\omega t}\\) in which all masses oscillate with the same frequency \\(\\omega\\) but with unknown phasors \\(z_i\\). Then the above equation becomes :\n\\[F_i = m\\ddot{x}_i = −m\\omega^2x_i = −k(2x_i − x_{i−1} − x_{i+1}) \\tag{4.6}\\]\nThis is one row of a matrix equation describing the entire system :\n\\[m\\omega^2x_i \\left(\\begin{array}{c} \\vdots \\\\ \\\\ x_i \\\\ \\\\ \\vdots \\end{array}\\right) =\n\\left(\\begin{array}{ccccccc} & & & \\vdots & & & \\\\ \\cdots & 0 & -1 & 2 & -1 & 0 & \\cdots \\\\ & & & \\vdots & & & \\end{array}\\right)\n\\left(\\begin{array}{c} \\vdots \\\\ x_{i-1} \\\\ x_i \\\\ x_{i+1} \\\\ \\vdots \\end{array}\\right)\n\\tag{4.7}\\]\nThis example is a typical eigenvalue problem, in that many of the matrix elements are zero, which can greatly simplify the computational challenge and make even large systems solvable. The matrix is symmetric, which means it is suitable for solving with our eigenproblem solving function above, or one of the solvers from scipy.linalg.\n\nm = np.array([[2, -1,  0,  0,  0,  0,  0],\n              [-1, 2, -1,  0,  0,  0,  0],\n              [0, -1,  2, -1,  0,  0,  0],\n              [0,  0, -1,  2, -1,  0,  0],\n              [0,  0,  0, -1,  2, -1,  0],\n              [0,  0,  0,  0, -1,  2, -1],\n              [0,  0,  0,  0,  0, -1,  2]])\n\nmus, vs = linalg.eig(m)\n\nThe eigenvalue associated with each mode gives the frequency, while the (complex) eigenvector provides the magnitude and phase of oscillation for each mass. We can plot the displacement of each mass as a function of time for each mode.\n\nimport numpy as np\nimport scipy.linalg as linalg\nimport matplotlib.pyplot as plt\n\n# a function to calculate the (real) displacement from complex phase\ndef disp(zi, omega, t):\n    return np.real(zi * np.exp(1j * omega * t))\n\n# set up some pretty colours for plotting\ncm  = plt.cm.viridis\ncol = [cm(int(x*cm.N/7)) for x in range(7)]\n\n# time period\nts = np.arange(0,40, 0.001)\n\n# loop over eigenmodes\nfor i in range(7):\n    \n    print(\"Mode       : \",i)\n    print(\"Eigenvalue : \", mus[i])\n\n    fig=plt.figure(figsize=(16, 4))\n\n    xs = []\n    \n    # loop over masses\n    for j in range(7):\n        \n        # get the displacement, and add an offset to separate out each line\n        offset = (2*j)-6\n        \n        # create displacement values from function using eigenvectors and eigenvalues\n        xs     = disp(vs.T[i][j], mus[i], ts) + offset\n        \n        # plot displacement\n        plt.plot(ts, xs, color=col[j])\n        \n        # plot central position to guide the eye\n        plt.plot([0, 40], [offset, offset], color=col[j], linestyle='dotted') \n\n    plt.xlabel(\"t\")\n    plt.show()\n\nMode       :  0\nEigenvalue :  (3.8477590650225726+0j)\nMode       :  1\nEigenvalue :  (3.4142135623730923+0j)\nMode       :  2\nEigenvalue :  (2.765366864730178+0j)\nMode       :  3\nEigenvalue :  (1.9999999999999984+0j)\nMode       :  4\nEigenvalue :  (0.1522409349774269+0j)\nMode       :  5\nEigenvalue :  (0.585786437626905+0j)\nMode       :  6\nEigenvalue :  (1.2346331352698203+0j)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Eigenproblems</span>"
    ]
  },
  {
    "objectID": "pde-1.html",
    "href": "pde-1.html",
    "title": "5  Boundary Value Problems",
    "section": "",
    "text": "5.1 Finite Difference Equations\nFinite difference equations for a given partial differential equation can be obtained using Taylor expansion. Using a 1-dimensional example for simplicity, we have a function \\(f(x)\\), represented on a set of points \\(x_i\\) with equal spacing \\(\\Delta(x)=h\\). We aim to find approximations for the derivatives of \\(f(x)\\). First we can write the Taylor expansion about \\(f(x+h)\\) :\n\\[f(x+h) = f(x)+ hf'(x)+\\frac{h^2}{2!}f''(x) + \\frac{h^3}{3!}f'''(x) + \\mathcal{O}(h^4) \\tag{5.1}\\]\nRe-arranging gives us an equation for \\(f'(x)\\) known as the “forward difference” approximation : \\[\nf'(x) =\\frac{f(x+h)-f(x)}{h}+\\mathcal{O}(h^2)\n\\tag{5.2}\\]\nOr in terms of our discrete grid of points : \\[\nf'(x_i) = \\frac{f(x_{i+1})-f(x_i)}{h}+\\mathcal{O}(h^2)\n\\tag{5.3}\\]\nIn a similar way, we can obtain the “backwards difference” approximation :\n\\[\nf'(x) =\\frac{f(x)-f(x-h)}{h}+\\mathcal{O}(h^2)\n\\tag{5.4}\\]\nAnd in terms of \\(x_i\\) : \\[\nf'(x_i) = \\frac{f(x_i)-f(x_{i-1})}{h}+\\mathcal{O}(h^2)\n\\tag{5.5}\\]\nThese are 1st order approximations, since the error is proportional to \\(h^2\\).\nFor the 2nd derivative, we can sum Taylor expansions about \\((x-h)\\) and \\((x+h)\\) to obtain :\n\\[\nf(x-h) + f(x+h) = 2f(x)+h^2f''(x)+\\mathcal{O}(h^4)\n\\tag{5.6}\\]\nSince the terms in \\(h^3\\) cancel.\nThis leads to \\[\nf''(x)=\\frac{f(x+h) - 2f(x)+f(x-h)}{h^2} + \\mathcal{O}(h^4)\n\\tag{5.7}\\]\nOr in terms of \\(x_i\\) : \\[\nf''(x)=\\frac{f(x_{i+1}) - 2f(x_i)+f(x_{i-1})}{h^2} + \\mathcal{O}(h^4)\n\\tag{5.8}\\]\nWe can use this approach to find a numerical approximation for any derivative, to any desired order. In some cases, we will have more than one equation, due to the “forward/backward” differences.\nNote that, when written in terms of \\(x_i\\) we are now dealing with sets of equations. If we have \\(N\\) points, i.e. \\(0 &lt; i &lt; N\\), then we have N equations, which can be written in matrix form.\nOnce we have a set of finite difference equations, we need a method to find a solution to them.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Boundary Value Problems</span>"
    ]
  },
  {
    "objectID": "pde-1.html#sec-relax",
    "href": "pde-1.html#sec-relax",
    "title": "5  Boundary Value Problems",
    "section": "5.2 Relaxation Methods",
    "text": "5.2 Relaxation Methods\nThe methods for finding a solution described below are all examples of “relaxation” methods. Here we start from an arbitrary set of values on the grid, and we repeatedly apply a recurrence relation (based on the finite difference equations) to find a solution. Iteration stops when some convergence condition has been met (typically based on the change in values between iterations). Note that this is conceptually similar to the time evolution of a system going from some non-equilibrium state to equilibrium.\nAlso, during each iteration we must ensure the boundary conditions are adhered to. For Dirichlet conditions, this will mean ensuring the value of boundary nodes are constant. For Neumann conditions it will mean ensuring differences between nodes are constant.\n\n5.2.1 Jacobi Method\nIn this method, we calculate a “new” value for each node from the finite difference equation, using “old” values as input. i.e. the recurrence relation is simply the full set of finite difference equations. This means we must store two copies of the grid at all times, but it does mean we can compute multiple nodes in parallel. (Actually achieving parallel processing is beyond the scope of this course !)\n\n\n5.2.2 Gauss-Seidel Method\nThis method is similar to the Jacobi method, but we store only one copy of the grid. This means each node is updated in turn, using a mixture of old and new grid values. While storing only one copy may be an advantage for large problems, this does mean that nodes cannot be computed in parallel.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Boundary Value Problems</span>"
    ]
  },
  {
    "objectID": "pde-1.html#successive-over-relaxation",
    "href": "pde-1.html#successive-over-relaxation",
    "title": "5  Boundary Value Problems",
    "section": "5.3 Successive Over-relaxation",
    "text": "5.3 Successive Over-relaxation\nThis is the same as the Gauss-Seidel method, but we now over-correct each node by some amount, controlled by a parameter \\(\\omega\\). This over-correction allows the method to converge more quickly for well chosen values of \\(\\omega\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Boundary Value Problems</span>"
    ]
  },
  {
    "objectID": "pde-2.html",
    "href": "pde-2.html",
    "title": "6  Diffusive Initial Value Problems",
    "section": "",
    "text": "6.1 Explicit Method\nIn this method, we use the “forward difference” version of the temporal 1st derivative, and the 2nd order spatial 2nd derivative, both derived in Section 5.1. This leads to\n\\[\n\\frac{u_i^{n+1} - u_i^n}{\\tau}=\\frac{u_{i+1}^n - 2u_i^n+u_{i-1}^n}{h^2}\n\\tag{6.2}\\]\nBy collecting terms in \\(u^{n+1}\\) and \\(u^n\\) we obtain\n\\[\nu_i^{n+1} = u_i^n + \\frac{\\tau}{h^2} (u_{i+1}^n - 2u_i^n+u_{i-1}^n)\n\\tag{6.3}\\]\nThis equation expresses the state of the system at timestep \\(n+1\\) (on the LHS) to those at the previous timestep \\(n\\) (on the RHS). If we know the state of the system at timestep zero (i.e. \\(u_i^0\\)) we can then determine the state at all future times by repeatedly applying the equation. Due to the choice of finite difference equations, this method is also known as a Forward Time Centred Space scheme. The updated value of a node at timestep \\(n+1\\) depends on its value at the previous timestep, along with those of its neighbours.\nUnfortunately, this method is not stable for all values of the parameters (\\(k\\), \\(\\tau\\), \\(h\\)). It can be shown that the condition for stability is : \\[\\frac{2 k \\tau}{h^2} &lt; 1\\]\nWhat this means physically is that the timestep \\(\\tau\\) must be less than the diffusion time across a spatial step \\(h\\). So in order to simulate spatial scales \\(\\lambda\\) (with \\(\\lambda &gt;&gt; h\\), we will need to run for at least \\(\\frac{\\lambda^2}{h^2}\\) timesteps., which can be prohibitively slow.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Diffusive Initial Value Problems</span>"
    ]
  },
  {
    "objectID": "pde-2.html#implicit-method",
    "href": "pde-2.html#implicit-method",
    "title": "6  Diffusive Initial Value Problems",
    "section": "6.2 Implicit Method",
    "text": "6.2 Implicit Method\nThe implicit method does not suffer from the stability issues of the explicit method. It differs in the choice of the finite difference equation used for the temporal 1st derivative. Here we use the “backwards difference” equation derived in Section 5.1.\n\\[\n\\frac{u_i^{n} - u_i^{n-1}}{\\tau}=\\frac{u_{i+1}^n - 2u_i^n+u_{i-1}^n}{h^2}\n\\tag{6.4}\\]\nCollecting terms in the same temporal index now gives\n\\[\nu_i^{n-1} = u_i^n - \\frac{\\tau}{h^2} (u_{i+1}^n - 2u_i^n+u_{i-1}^n)\n\\tag{6.5}\\]\nNow we have an equation which seems to do the opposite of the previous method - if we apply it repeatedly, we step backwards in time. And indeed, this method is known as Backwards Time Centred Space.\nIt might seem that this equation is unhelpful, since we generally know the initial state of the system and want to predict its evolution into the future. However, this is a linear set of equations. In order to transform the system from timestep \\(n\\) to \\(n+1\\) we now have to solve that set of linear equations. It helps to write the set of equations in matrix form :\n\\[u^{n-1} = A u^n{n} \\tag{6.6}\\]\nHere \\(u^{n+1}\\) and \\(u^n\\) are both vectors containing the spatial elements of the grid at a single timestep, and the elements of matrix \\(A\\) are given by :\n\\[\n\\begin{aligned}\nA_{j,j} &= 1 + 2 \\alpha \\\\\nA_{j,j+1} &= A_{j+1,j} = - \\alpha\n\\end{aligned}\n\\tag{6.7}\\]\nWhere \\(\\alpha = \\frac{k \\tau}{h^2}\\). In a simplified version with only 4 spatial indices, \\(A\\) would look like this :\n\\[\n\\begin{array}{cccc}\n1+2\\alpha & -\\alpha & 0 & 0 \\\\\n-\\alpha & 1+2\\alpha & -\\alpha & 0 \\\\\n0 & -\\alpha & 1+2\\alpha & -\\alpha \\\\\n0 & 0 & -\\alpha & 1+2\\alpha \\\\\n\\end{array}\n\\tag{6.8}\\]\nOnce we have constructed the matrix, we can obtain \\(u^{n+1}\\) from \\(u^n\\), by using the techniques described in Chapter 3 to solve Equation 7.6.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Diffusive Initial Value Problems</span>"
    ]
  },
  {
    "objectID": "pde-2.html#crank-nicholson-method",
    "href": "pde-2.html#crank-nicholson-method",
    "title": "6  Diffusive Initial Value Problems",
    "section": "6.3 Crank-Nicholson Method",
    "text": "6.3 Crank-Nicholson Method\nEquation 7.4 used in the Implicit method can be written in terms of \\(u^n\\) and \\(u^{n+1}\\) (as for the Explicit method), i.e. :\n\\[\n\\frac{u_i^{n+1} - u_i^{n}}{\\tau}=\\frac{u_{i+1}^{n+1} - 2u_i^{n+1}+u_{i-1}^{n+1}}{h^2}\n\\tag{6.9}\\]\nThe Crank-Nicholson method is obtained by replacing the RHS with the average of the RHS in equations Equation 7.2 and Equation 6.9.\n\\[\n\\frac{u_i^{n+1} - u_i^{n}}{\\tau}=\\frac{1}{h^2} (u_{i+1}^{n+1} - 2u_i^{n+1}+u_{i-1}^{n+1} + u_{i+1}^n - 2u_i^n+u_{i-1}^n)\n\\tag{6.10}\\]\nLike the Implicit method, this method is unconditionally stable, and the state at each timestep must be obtained by solving the linear set of equations. Writing the equation above as a matrix equation is left as an exercise for the interested reader.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Diffusive Initial Value Problems</span>"
    ]
  },
  {
    "objectID": "mc-1.html",
    "href": "mc-1.html",
    "title": "7  Pseudo Random Number Sampling",
    "section": "",
    "text": "7.1 Uniform Probability Distributions\nWe will often need uniform probability distributions over an interval other than \\([0,1)\\). It is straightforward to map this interval to the desired one, as shown in the example below.\nimport numpy.random as random\nimport matplotlib.pyplot as plt\n\n# produce random numbers in the range 150-250\na = 100*random.random(int(1e5))+150\nplt.hist(a)\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Pseudo Random Number Sampling</span>"
    ]
  },
  {
    "objectID": "mc-1.html#analytical-method",
    "href": "mc-1.html#analytical-method",
    "title": "7  Pseudo Random Number Sampling",
    "section": "7.2 Analytical Method",
    "text": "7.2 Analytical Method\nThe above transformation is a special case of the method described in this section. We can consider a random number generator that produces values x over the interval \\((x_1, x_2)\\) with probability \\(P(x)\\), which we wish to convert to values y on the interval \\((y_1, y_2)\\), with probability \\(P'(y)\\). To construct a transformation from a generated value \\(x_{in}\\) to an output value with the required distribution, \\(y_{out}\\), we require that the cumulative distributions are equal :\n\\[\\int_{x_0}^{x_{in}} P(x) dx = \\int_{y_0}^{y_{out}} P'(y) dy \\tag{7.1}\\]\nNote that for \\(x_{in} = x_2\\), \\(y_{out} = y_2\\) both integrals must equal 1.\nIf the LHS of the above equation is uniform on the interval \\([0,1)\\), then we have :\n\\[x_{in} = \\int_{y_0}^{y_{out}} P'(y) dy \\tag{7.2}\\]\nIf we then define the function \\(Q(y_{out})\\) such that :\n\\[Q(y_{out}) = x_{in} = \\int_{y_0}^{y_{out}} P'(y) dy \\tag{7.3}\\]\nThen the transformation we must apply to \\(x_{in}\\), in order to obtain \\(y_{out}\\), is simply the inverse function, ie :\n\\[y_{out} = Q^{-1}(x_{in}) \\tag{7.4}\\]",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Pseudo Random Number Sampling</span>"
    ]
  },
  {
    "objectID": "mc-1.html#analytical-method-example",
    "href": "mc-1.html#analytical-method-example",
    "title": "7  Pseudo Random Number Sampling",
    "section": "7.3 Analytical Method Example",
    "text": "7.3 Analytical Method Example\nIn this example, we will write a function to produce values \\(y\\) in the interval \\([0, \\pi)\\) with probability distribution proportional to \\(\\sin(y)\\).\nHere, the integral above becomes :\n\\[\n\\begin{aligned}\nQ(y_{out}) &= \\frac{1}{2}\\int_{0}^{y_{out}} \\sin(y) dy \\\\\n           &= -\\frac{1}{2}\\cos(y_{out}) + C\n\\end{aligned}\n\\tag{7.5}\\]\nNote the factor \\(\\frac{1}{2}\\) is required to ensure the integral from \\(0\\) to \\(\\pi\\) is equal to 1. We can determine the constant of integration by requiring that \\(Q(0) = 0\\), for \\(x_{in}=0\\) and \\(Q(\\pi)=1\\) for \\(x_{in}=1\\).\nWe then find that\n\\[Q(y_{out}) = -\\frac{1}{2}\\cos(y_{out}) + \\frac{1}{2} \\tag{7.6}\\]\nAnd our inverse transformation is :\n\\[y_{out} = Q^{-1}(x_{in}) = \\cos^{-1}(1-2x_{in})  \\tag{7.7}\\]\n\ndef randSinAna():\n    \"\"\"Generate a random theta between 0 and pi, with PDF sin(theta) using analytical method\"\"\"\n    x = random.random()\n    return np.arccos(1-2*x)\n\n\nimport numpy as np\n\n# generate 50,000 points using a list comprehension\nn1s = [randSinAna() for _ in range(50000)]\n\n# plot a histogram\nhist1, bins1, patches1 = plt.hist(n1s, bins=50, density=True, label=\"Analytic Method\")\n\n# plot a sin(theta) function for comparison\nbin_centres = (bins1[1:] + bins1[:-1])/2\nplt.plot(bin_centres, np.sin(bin_centres)/2, label=r'$sin(\\theta)$')\n\nplt.xlabel(r'$\\theta$')\nplt.ylabel(r'$P(\\theta)$')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nIt might be worth highlighing the method used here (and in examples below) to generate a large number of points. This is a list comprehension. The list is generated by calling the first argument in the square brackets (here randSinAna()) for every iteration of the for loop. The for loop uses the underscore instead of a loop variable, since no variable is required. But in other cases, one could use a standard for loop to generate values in a list, eg :\n\n[i for i in range(5)]\n\n[0, 1, 2, 3, 4]",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Pseudo Random Number Sampling</span>"
    ]
  },
  {
    "objectID": "mc-1.html#acceptreject-method",
    "href": "mc-1.html#acceptreject-method",
    "title": "7  Pseudo Random Number Sampling",
    "section": "7.4 Accept/Reject Method",
    "text": "7.4 Accept/Reject Method\nFor some PDFs, the integral required by the previous method cannot be determined analytically. In such cases, the accept/reject method provides a simple alternative. This method involves three steps : 1. a random number, \\(y\\), is generated in the desired interval \\((y_1, y_2)\\) 2. a second random number, \\(p\\), is generated between 0 and the maximum value of \\(P'(y)\\) 3. if \\(p &lt; P'(y)\\) then \\(y\\) is returned, otherwise it is rejected and the process is repeated\nThis method is clearly less efficient than the analytical method, since two random numbers are generated for each number returned, and some fraction of these are rejected. However, it allows us to generate any arbitrary probability distribution.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Pseudo Random Number Sampling</span>"
    ]
  },
  {
    "objectID": "mc-1.html#acceptreject-example",
    "href": "mc-1.html#acceptreject-example",
    "title": "7  Pseudo Random Number Sampling",
    "section": "7.5 Accept/Reject Example",
    "text": "7.5 Accept/Reject Example\nHere we demonstrate the accept/reject method for the same example as above, to produce values \\(y\\) in the interval \\([0, \\pi)\\), with probability distribution proportional to \\(\\sin(y)\\).\n\ndef randSinAR():\n    \"\"\"Generate a random theta between 0 and pi, with PDF sin(theta) using accept/reject method\"\"\"\n    while True:\n        x = np.pi * np.random.random()\n        y = np.random.random()\n        if y &lt; np.sin(x):\n            return x\n        else:\n            continue\n\n\n# generate 50,000 points using a list comprehension\nn2s = [randSinAR() for _ in range(50000)]\n\n# plot a histogram from the analytic method\nhist1, bins1, patches1 = plt.hist(n1s, bins=50, density=True, label=\"Analytic Method\")\n\n# plot another histogram from the accept/reject method\nhist2, bins2, patches2 = plt.hist(n2s, bins=bins1, density=True, label=\"Accept/reject\")\n\n# and the sin(theta) function for comparison\nplt.plot(bin_centres, np.sin(bin_centres)/2, label=r'$sin(\\theta)$')\n\nplt.xlabel(r'$\\theta$')\nplt.ylabel(r'$P(\\theta)$')\nplt.legend()\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Pseudo Random Number Sampling</span>"
    ]
  },
  {
    "objectID": "mc-2.html",
    "href": "mc-2.html",
    "title": "8  Multivariate Sampling",
    "section": "",
    "text": "8.1 Analytic Example\nAn analytic method for the unit disc problem needs to ensure that the density of points is constant over the disc, ie that \\(P(x,y) \\propto dA\\) for area element \\(dA\\). In polar coordinates, we can write this as :\n\\[P(x,y) \\propto dA = r dr d\\phi\\]\nSince we will start by generating values with uniform distributions (let’s say \\(u\\) and \\(v\\)), we want to obtain transformations \\((u,v) \\rightarrow (r, \\phi)\\) such that :\n\\[dA = r dr d\\phi= du dv\\]\nClearly these substitutions are sufficient :\n\\[du = r dr\\] \\[dv = d\\phi\\]\nClearly we can just generate \\(\\phi\\) with a uniform distribution. The function to produce \\(r\\) from uniformly distributed \\(u\\) is obtained by integration :\n\\[u = \\frac{1}{2}r^2\\]\nand\n\\[r = \\sqrt{2u}\\]\nHowever, this will produce a disc with incorrect area. The required area is \\(\\pi\\), so we can obtain the constant of integration by requiring \\(\\int dA = \\pi\\), which gives :\n\\[r = \\sqrt{u}\\]\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef unitDiscAna():\n    phi = 2 * np.pi * np.random.random()\n    r = np.sqrt(np.random.random())\n    \n    # convert to cartesian coordinates\n    x = r * np.cos(phi)\n    y = r * np.sin(phi)\n    \n    return np.array([x, y])\n\nps = np.array([unitDiscAna() for _ in range(1000)])\n\nplt.axis('equal')\nplt.scatter(ps[:,0], ps[:,1], marker='.', c='r')\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Multivariate Sampling</span>"
    ]
  },
  {
    "objectID": "mc-2.html#acceptreject-example",
    "href": "mc-2.html#acceptreject-example",
    "title": "8  Multivariate Sampling",
    "section": "8.2 Accept/Reject Example",
    "text": "8.2 Accept/Reject Example\nSuppose we want to randomly generate points \\((x,y)\\) within a unit disc. A simple approach is to generate uniform distributions of \\(x\\) and \\(y\\) separately, and then use an accept/reject method to remove any points not in the disc (ie. where \\(\\sqrt{x^2 + y^2} \\gt 1\\)). This is illustrated in the example below.\n\ndef unitDiscAR():\n    x = 2 * np.random.random() - 1\n    y = 2 * np.random.random() - 1\n    while np.sqrt(x**2+ y**2) &gt; 1:\n        x = 2 * np.random.random() - 1\n        y = 2 * np.random.random() - 1\n    return np.array([x, y])\n\nps = np.array([unitDiscAR() for _ in range(1000)])\n\nplt.axis('equal')\nplt.scatter(ps[:,0], ps[:,1], marker='.', c='r')\nplt.show()\n\n\n\n\n\n\n\n\nNote that since the unitDisc() method returns a vector, we need to :\n\nconvert the list generated by the list comprehension (which calls unitDisc() many times) into a 2D array\nuse array slicing to obtain arrays of \\(x\\) and \\(y\\) values separately when plotting, ie. ps[:,0] gives a 1D array of \\(x\\) values.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Multivariate Sampling</span>"
    ]
  }
]