---
title: "Simultaneous Equations"
format: html
label: #sec-lineqs
execute:
  error: true
---

Many problems in physics require solving simultaneous equations. When these become large and complex, numerical routines are required.

A set of simultaneous equations can always be written in matrix form, for example, two equations in two unknowns ($x_1$ and $x_2$)

$$
\begin{aligned}
ax_1 + bx_2 &= y_1 \\
cx_1 + dx_2 &= y_2
\end{aligned}
$$ {#eq-a}

can be rewritten as

$$
\left(\begin{array}{cc} a & b \\ c & d\end{array}\right)
\left(\begin{array}{c} x_1 \\ x_2 \end{array}\right) = 
\left(\begin{array}{c} y_1 \\ y_2 \end{array}\right)
$$ {#eq-b}

An arbitrary set of equations is

$$Ax = y$$  {#eq-c}

where A is the matrix of coefficients, x is the vector of unknown variables $x_1$, $x_2$, ... and y is the known vector of constants.


## Inverse Matrix

One way to solve the above equation is to multiply both sides by the inverse of A:

$$A^{-1} A x = A^{-1} y$$ {#eq-d}

giving :

$$x = A^{-1} y$$  {#eq-e}

This is demonstrated in the example below for a simple test case :

$$
\left(\begin{array}{ccc} 1 & 2 & 2 \\
                        3 & 1 & 6 \\
                        0 & 2 & 2\end{array}\right)
\left(\begin{array}{c} x_1 \\ x_2 \\ x_3\end{array}\right) = 
\left(\begin{array}{c} 2 \\ 7 \\ 1\end{array}\right)
$$  {#eq-f}

```{python}
import numpy as np
import scipy.linalg as linalg

def solve_inv(a,y):
    x = linalg.inv(a) @ y
    return x

a = np.array([[1, 2, 2,],[3, 1, 6],[0, 2, 2]])
y = np.array([[2], [7], [1]])
print(a)
print(y)

x = solve_inv(a,y)
print(x)
```

Is this the solution? We can easily check by inserting the solution into the original equation.

```{python}
print(a @ x)
```

Which is indeed equal to our `y` above. This kind of test is known as a 'closure test' and will be used frequently throughout this unit to verify our code.

Before using this method for solving simultaneous equations, though, we should understand how `scipy.linalg.inv` finds the matrix inverse.  Unfortunately, this is tricky to understand from the reference page, ([scipy.linalg.inv](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.inv.html)). But, if you examine the source code for this function you'll see that it uses a function called DGETRI. This is defined in the LAPACK library, and its reference page is [here](https://www.netlib.org/lapack/explore-html-3.6.1/dd/d9a/group__double_g_ecomputational_ga56d9c860ce4ce42ded7f914fdb0683ff.html).  As you can see this routine uses LU decomposition to find the inverse! It doesn't make sense, therefore, to find a matrix inverse simply to solve a simultaneous equation, and using LU decomposition directly will involve fewer operations.  However, there are exceptions when dealing with many simultaneous equations. For example, suppose you have a sequence of problems which all feature the same matrix $A$, but have different RHS $y$. In this case it would be efficient to invert $A$ once, then multiply by $y$ to solve each problem, since multiplication involves fewer operations than LU decomposition.

As an aside, LAPACK is a linear algebra library written in FORTRAN - which remains one of the most efficient languages for writing numerical methods - and most routines in `scipy.linalg` basically provide a Python interface to this library.


## Gaussian Elimination

Some sets of simultaneous equations are easy to solve.  For example :

$$
\begin{aligned}
a x_1 &= y_1 \\
b x_2 &= y_2 \\
c x_3 &= y_3
\end{aligned}
$$  {#eq-g}
This can be written in what is known as row echelon form :

$$
\left(\begin{array}{ccc} a & 0 & 0 \\
                        0 & b & 0 \\
                        0 & 0 & c\end{array}\right)
\left(\begin{array}{c} x_1 \\ x_2 \\ x_3\end{array}\right) = 
\left(\begin{array}{c} y_1 \\ y_2 \\ y_3\end{array}\right)
$$  {#eq-h}

And then reduced row echelon form :

$$
\left(\begin{array}{ccc} 1 & 0 & 0 \\
                        0 & 1 & 0 \\
                        0 & 0 & 1\end{array}\right)
\left(\begin{array}{c} x_1 \\ x_2 \\ x_3\end{array}\right) = 
\left(\begin{array}{c} y_1/a \\ y_2/b \\ y_3/c\end{array}\right)
$$  {#eq-i}

Gauss-Jordan elimination is a process which reduces any linear equation set to this form. It can be shown that the reduced row echelon form is unique, and therefore independent of the order of operations which are used to find it. The technique is illustrated using the example problem from earlier. It's convenient to use the 'augmented' matrix, which includes the right-hand side.
$$
\left(\begin{array}{ccc|c} 
    1 & 2 & 2 & 2 \\
    3 & 1 & 6 & 7 \\
    0 & 2 & 2 & 1
\end{array}\right)
$$  {#eq-j}

Then we apply simple operations until we obtain the equation in row echelon form. These operations include:

- Multiply a row by a constant
- Swap two rows
- Sum two rows in a linear combination

(Hopefully, these sound reasonably familiar - we are just formalising techniques you will have used before)

Replace $R_1$ (row 1) with $R_1 - R_3$ :
$$
\left(\begin{array}{ccc|c} 
    1 & 0 & 0 & 1 \\
    3 & 1 & 6 & 7 \\
    0 & 2 & 2 & 1
\end{array}\right)
$$  {#eq-k}

Replace $R_2$ with $R_2 - 3R_1$ :
$$
\left(\begin{array}{ccc|c} 
    1 & 0 & 0 & 1 \\
    0 & 1 & 6 & 4 \\
    0 & 2 & 2 & 1
\end{array}\right)
$$  {#eq-l}

Replace $R_2$ with $R_2 - \frac{2}{5}R_2$ :
$$
\left(\begin{array}{ccc|c} 
    1 & 0 & 0 & 1 \\
    0 & -5 & 0 & 1 \\
    0 & 0 & 2 & \frac{7}{5}
\end{array}\right)
$$  {#eq-m}

And then finally for reduced row echelon form, replace $R_2$ with $\frac{-1}{5}R_2$ and $R_3$ with $\frac{1}{2}R_3$
$$
\left(\begin{array}{ccc|c} 
    1 & 0 & 0 & 1 \\
    0 & 1 & 0 & -\frac{1}{5} \\
    0 & 0 & 1 & \frac{7}{10}
\end{array}\right)
$$  {#eq-n}

So the solution is :
$$
\begin{aligned}
x_1 &= 1 \\
x_2 &= -\frac{1}{5} \\
x_3 &= \frac{7}{10}
\end{aligned}
$$  {#eq-o}

## LU Decomposition

Matrix decomposition techniques involve factorising a general matrix into a product of several matrices. LU decomposition involves writing the general matrix, $A$, as the product of two triangular matrices, $L$ and $U$.

$$
A=
\left(\begin{array}{ccc} a_{11} & a_{12} & a_{13} \\
                         a_{21} & a_{22} & a_{23} \\
                         a_{31} & a_{32} & a_{33}
\end{array}\right)
$$  {#eq-p}

$$
A=LU=
\left(\begin{array}{ccc} 1      & 0      & 0 \\
                         l_{21} & 1      & 0 \\
                         l_{31} & l_{32} & 1
\end{array}\right)
\left(\begin{array}{ccc} u_{11} & u_{12} & u_{13} \\
                         0      & u_{22} & u_{23} \\
                         0      & 0      & u_{33}
\end{array}\right)
$$  {#eq-q}

We can use LU decomposition to solve matrix equations since it allows us to write the equation $$Ax = y$$ as $L(Ux)=y$.  This can then be written as two equations $Lc=y$ and $Ux=c$, which are trivially solved, first for $c$, and then for $x$.

The matrices $L$ and $U$ can be found using the operations described above for Gaussian elimination. There are several algorithmic formulations that define the sequence of operations. Scipy provides an LU decomposition routine: `scipy.linalg.lu()`. Note that this performs a variation on the LU decomposition described above, since it also computes a permutation matrix $P$, such that 

$$PA = LU$$  {#eq-r}

Scipy also provides a simple function to obtain the solutions to a matrix equation. `scipy.linalg.lu_solve()` expects the $L$, $U$ and $P$ matrices as arguments, as shown in the example below.

- [scipy.linalg.lu](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.lu.html)

- [scipy.linalg.lu_solve](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.lu_solve.html)

```{python}
def solve_lu(a,y):
    lu, piv = linalg.lu_factor(a)
    x = linalg.lu_solve((lu, piv), y)
    return x

print(solve_lu(a,y))
```

Note that the general purpose solvers provided by both numpy and scipy both utilise LU decomposition :

- [numpy.linalg.solve](https://numpy.org/doc/stable/reference/generated/numpy.linalg.solve.html)

- [scipy.linalg.solve](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.solve.html)


## SVD Decomposition

LU decomposition will find an exact solution to the matrix equation in a wide variety of cases.  However, a solution may not exist, or there may be infinite solutions. In such cases, the Singular Value Decomposition may be of use.

For an $m \times n$ matrix $A$, the singular values, $\sigma$ are given by the solutions to

$$
\begin{aligned}
Av &= \sigma u \\
A^Tu &=\sigma v
\end{aligned}
$$  {#eq-s}

where $u$ and $v$ are two non-zero vectors.  These equations are closely related to the eigenvalue equation.  Indeed, the singular values are also the square roots of the eigenvalues of $A^TA$.

The singular value decomposition of $A$ is

$$A = U\Sigma V^T$$  {#eq-t}

where $U$ and $V$ are orthonormal matrices, and $\Sigma$ is a matrix with the singular values on its leading diagonal, and zero elsewhere.

The SVD decomposition allows use to compute the pseudo-inverse of $A$, which is given by :

$$A^\dagger = V \Sigma^\dagger U^T$$  {#eq-u}

where $\Sigma^\dagger$ is the pseudo-inverse of $\Sigma$ and is obtained by transposing $\Sigma$ and replacing each non-zero element with it's reciprocal.

The pseudoinverse (also known as the Moore-Penrose inverse) can always be computed, even when $A$ is singular, ie. when when $\frac{1}{|A|}=0$ and the inverse cannot be found.

In the context of solving a matrix equation $Ax=y$, the product of pseudoinverse and the RHS, (i.e. $\bar{X}=A^\dagger y$) has various properties. When A is non-singular, $\bar{x}$ gives the solution to $Ax=y$.  When $A$ is singular, $\bar{x}$ is a least squares approximation to the nearest solution.  When $Ax=y$ has a space of solutions (equivalent to a set of simultaneous equations with degeneracy), then $\bar{x}$ is a vector which describes this space.

SVD decomposition is available in Scipy using `scipy.linalg.svd()`. For further information, see [scipy.linalg.svd](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.svd.html). Note that, unlike LU decomposition, no `solve()` function is supplied, and instead we must write some code to calculate $\bar{x}$.

```{python}
def solve_svd(a,y):
    u, s, v = linalg.svd(a)
    x = v.T @ np.diag(1/s) @ u.T @ y
    return x

print(solve_svd(a,y))
```

Alternatively, a function to compute the pseudoinverse directly is provided, [scipy.linalg.pinv](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.pinv.html).

```{python}
print( linalg.pinv(a) @ y)
```

## Physics Example

Here we illustrate the use of simultaneous equation solvers in a familiar context - the use of Kirchoff's laws and Ohm's law to understand resistor networks. Typically, analysis of a resistor network will involve solving simultaneous equations, to calculate voltage and current at the desired points in the network. Consider the electronic circuit shown in the diagram.

![Figure 1 - Example resistor network.](circuit.png)

Where : $V_1 = 12V$, $V_2 = 12V$, $R_1 = 3 \Omega$, $R_2 = 3 \Omega$, $R_3 = 10 \Omega$, $R_4 = 2 \Omega$, $R_5 = 2 \Omega$.

By identifying the three current loops indicated, we can use Kirchoff's loop rule and Ohm's law to write : 
$$
\begin{aligned}
V_1 &= I_1 R_1 + (I_1 - I_2) R_2 \\
0   &= (I_2 - I_1) R_2 + I_2 R_3 + (I_2 - I_3) R_4 \\
-V_2 &= (I_3 - I_2) R_4 + I_3 R_5
\end{aligned}
$$  {#eq-v}

So we have a set of simultaneous equations, which we can write as a matrix equation :

$$
\begin{pmatrix}
R_1+R_2 & -R_2 & 0 \\
-R_2 & R_2+R_3+R_4 & -R_4 \\
0  & -R_4 & R_4+R_5 \\
\end{pmatrix}
\begin{pmatrix}
I_1 \\
I_2 \\
I_3
\end{pmatrix}
=
\begin{pmatrix}
V_1 \\
0 \\
-V_2
\end{pmatrix}
$$  {#eq-w}

Solving this matrix equation will provide the current at all points in the circuit. This method is known as "mesh analysis" of circuits.

We can write a simple function that, given the voltage and resistor values, will return the currents :

```{python}
def meshAnalysis(v1, v2, r1, r2, r3, r4, r5):
    m = np.array ([[r1+r2, -r2, 0],[-r2,r2+r3+r4,-r4],[0,-r4,r4+r5]])
    v = np.array([[v1],[0],[-v2]])
    i = linalg.solve(m,v)
    return i
```

Which, for the values given, will return the three currents :

```{python}
i = meshAnalysis(12, 12, 3, 3, 10, 2, 2)

print(i)
```

Or we could calculate, for example, how $I_2$ will vary as a function of $R_4$, with all other values fixed :

```{python}
r4s = np.linspace(0.1, 5.0, 100)
i2s = np.empty(len(r4s))

for j,r4 in enumerate(r4s):
    i     = meshAnalysis(12, 12, 3, 3, 10, r4, 2)
    i2s[j] = i[1][0]

import matplotlib.pyplot as plt

plt.plot(r4s, i2s)
plt.ylabel('$I_2$ (A)')
plt.xlabel('$R_4$ (ohm)')
plt.show()
```
