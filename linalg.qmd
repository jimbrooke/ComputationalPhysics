---
title: "Linear Algebra"
format: html
execute:
  error: true
---

## Basic Matrix Operations


{{< pagebreak >}}



{{< pagebreak >}}

### Comparison of Algorithms

There are a variety of reasons that might influence our choice of algorithm, which will depend on the nature of the problem we are trying to solve.

One interesting comparison we can perform is to consider solutions to a near-singular matrix problem such as the one below for small $k$ :

```{=tex}
\begin{eqnarray*}
x + y + z & = & 5 \\
x + 2y - z & = & 10 \\
2x + 3y +kz & = & 15 
\end{eqnarray*}
```
Note that when $k=0$ the equations are degenerate and there is a space of solutions.

We will scan a wide range of values of $k$, use each of the above methods to find a solution, and plot a measure of the numerical error. (To be specific, we insert the obtained solution into the equation above, find the difference with respect to the RHS ie. $ax - y$, and take the maximum value of this vector).

However, first we must add some protection to the inverse matrix method for errors when $k$ is 'too small' and the matrix is effectively singular.

```{python}
def solve_inv(a,y):
    try:
        x = linalg.inv(a) @ y
    except linalg.LinAlgError as err:
        x = np.full_like(y,np.NaN)
    return x
```

```{python}
import matplotlib.pyplot as plt

ks = np.logspace(-20, 5, 250)
p1s = []
p2s = []
p3s = []

y = np.array([5,10,15])

# this code snippet is just to suppress some warnings
import warnings
with warnings.catch_warnings():
    warnings.simplefilter("ignore")
    
    # loop over values of k
    for k in ks:
        
        # compute matrix for this value of k
        a = np.array([[1,1,1],[1,2,-1],[2,3,k]])
        
        # find the error for inverse method
        diff1 = a @ solve_inv(a,y) - y
        p1s.append(abs(np.amax(diff1)))

        # find the error for LU decomposition
        diff2 = a @ solve_lu(a,y) - y
        p2s.append(abs(np.amax(diff2)))

        # find the error for SVD decomposition
        diff3 = a @ solve_svd(a,y) - y
        p3s.append(abs(np.amax(diff3)))
        
# LU solver will only result in 0 or NaN - replace these with min/max y values respectively, using a list comprehension
new_p2s = [1.1e-18 if x==0 else 0.8e-5 for x in p2s]

fig=plt.figure(figsize=(8, 8))
plt.rcParams.update({'font.size': 12})
plt.plot(ks, p1s, color='r', label="Inverse")
plt.plot(ks, p3s, color='b', label="SVD")
plt.plot(ks, new_p2s, color='g', label="LU")
plt.yscale("log")
plt.ylabel("error")
plt.xscale("log")
plt.xlabel("k")
plt.ylim(1e-18, 1e-5)
plt.legend()
plt.show()
```

Here are some observations and explanations of what we see in this plot.

**1. Matrix inverse**.

At best (eg. for k\>1) this method generates errors of order machine epsilon (around 1e-15). These can be understood as rounding errors, due to limited floating point precision. But for k\<1, the error is inversely proportional to k. This can be understood since calculating the matrix inverse involves dividing by the determinant of the matrix. The determinant is O(1/k), and the error in the division can be of the same magnitude.

Note that if we are "lucky" there is no rounding error - when the values used in the matrix, and its inverse, can all be exactly represented by floating point values, despite their limited precision. This explains the steps in the plot, which extend down to zero error.

**2. LU decomposition**

This method either fails altogether, or has zero error. The solver checks if the matrix is effectively singular (which it is for $k$ below machine epsilon, ie. $k<10^{-15}$) and returns NaN. For $k>10^{-15}$, no error is introduced because the method (effectively Gaussian elimination) does not involve operations which introduce a rounding error in the context of the original problem (ie. no division by large/small numbers).

**3. SVD decomposition**

This method intrinsically returns a meaningful result for singular matrices. If the problem is degenerate, the result can be interpreted in terms of a space of solutions. If the problem is truly singular, then the result is effectively the 'least squares' closest approximate result. Hence we see rounding error of order the system epsilon for small values, even when k is smaller than the system epsilon and the problem cannot be solved by LUD.

{{< pagebreak >}}

### Physics Example

Here we illustrate the use of simultaneous equation solvers in a familiar context - the use of Kirchoff's laws and Ohm's law to understand resistor networks. Typically, analysis of a resistor network will involve solving simultaneous equations, to calculate voltage and current at the desired points in the network. Consider the electronic circuit shown in the diagram.

![Figure 1 - Example resistor network.](circuit.png)

Where : $V_1 = 12V$, $V_2 = 12V$, $R_1 = 3 \Omega$, $R_2 = 3 \Omega$, $R_3 = 10 \Omega$, $R_4 = 2 \Omega$, $R_5 = 2 \Omega$.

By identifying the three current loops indicated, we can use Kirchoff's loop rule and Ohm's law to write : 
$$
\begin{align}
V_1 &= I_1 R_1 + (I_1 - I_2) R_2 \\
0   &= (I_2 - I_1) R_2 + I_2 R_3 + (I_2 - I_3) R_4 \\
-V_2 &= (I_3 - I_2) R_4 + I_3 R_5
\end{align}
$$

So we have a set of simultaneous equations, which we can write as a matrix equation :

$$
\begin{pmatrix}
R_1+R_2 & -R_2 & 0 \\
-R_2 & R_2+R_3+R_4 & -R_4 \\
0  & -R_4 & R_4+R_5 \\
\end{pmatrix}
\begin{pmatrix}
I_1 \\
I_2 \\
I_3
\end{pmatrix}
=
\begin{pmatrix}
V_1 \\
0 \\
-V_2
\end{pmatrix}
$$

Solving this matrix equation will provide the current at all points in the circuit. This method is known as "mesh analysis" of circuits.

We can write a simple function that, given the voltage and resistor values, will return the currents :

```{python}
def meshAnalysis(v1, v2, r1, r2, r3, r4, r5):
    m = np.array ([[r1+r2, -r2, 0],[-r2,r2+r3+r4,-r4],[0,-r4,r4+r5]])
    v = np.array([[v1],[0],[-v2]])
    i = linalg.solve(m,v)
    return i
```

Which, for the values given, will return the three currents :

```{python}
i = meshAnalysis(12, 12, 3, 3, 10, 2, 2)

print(i)
```

Or we could calculate, for example, how $I_2$ will vary as a function of $R_4$, with all other values fixed :

```{python}
r4s = np.linspace(0.1, 5.0, 100)
i2s = np.empty(len(r4s))

for j,r4 in enumerate(r4s):
    i     = meshAnalysis(12, 12, 3, 3, 10, r4, 2)
    i2s[j] = i[1]

import matplotlib.pyplot as plt

plt.plot(r4s, i2s)
plt.ylabel('$I_2$ (A)')
plt.xlabel('$R_4$ ($\Omega$)')
plt.show()
```

{{< pagebreak >}}

## Eigenproblems

A square NxN matrix $A$, has eigenvector $u$ and eigenvalue $\lambda$ that satisfy :

$$(A - \lambda I)u = 0$$

You may be familiar with one method for finding eigenvalues, which is to find the roots of the N-th degree polynomial found by expanding :

$$p(t) = \det{|A - t I|} = 0$$

Finding eigenvalues is closely connected to finding roots of polynomials. Here we will explore some iterative methods for finding both eigenvalues and eigenvectors.

Many eigenproblem solving routines are provided by SciPy. In particular, `scipy.linalg.eig(A)` will return a tuple containing the eigenvalues and eigenvectors of A.  If only the eigenvalues are required, `scipy.linalg.eigenvals(A)` can be used.

Care should be taken when using `scipy.linalg.eig`, since it will find "left" and "right" eigenvectors, as specified, which are the solutions to $v A = \lambda v$ and $A v = \lambda v$ respectively.

For further details see https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.eig.html

Additional routines include :
  * https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.eigh.html (for Hermitian matrices)
  * https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.eig_banded.html (for banded matrices)
  * https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.eigs.html (for sparse, square symmetric matrices)

### Simple example

We can test these sovlers using the matrix :

$$A = 
\pmatrix{
-2 & -4 & 2 \\
-2 &  1 & 2 \\
4  &  2 & 5}
$$

for which the eigenvalues are $\lambda^{(0)}=6$, $\lambda^{(1)}=-5$, $\lambda^{(2)}=3$.

Note that the algorithms discussed here will all find _unit_ eigenvectors.  The eigenvector corresponding to $\lambda^{(0)}$ is then :

$$\hat{u}^{(0)}=\pmatrix{\frac{1}{\sqrt{293}} \\
\frac{6}{\sqrt{293}} \\
\frac{16}{\sqrt{293}}
}
=
\pmatrix{0.058 \\
0.351 \\
0.935}$$

with numerical values given to 3 decimal places on the RHS.

```{python}
import numpy as np
import scipy.linalg as linalg

m = np.array([[-2,-4,2],[-2,1,2],[4,2,5]])

# set seed for repeatability
np.random.seed(2)

# run the algorithm
mus, vs = linalg.eig(m)

# print results
np.set_printoptions(precision=3)
for i in range(3):
    print("Eigenvalue/vector : {:.1f} {}".format(mus[i], vs.T[i]))
```

Which includes the solution expected.

Note that we have transposed the array of eigenvectors returned by `linalg.eig()`. This is a feature of the function, as described in the reference manual : _"The normalized left eigenvector corresponding to the eigenvalue `w[i]` is the column `vl[:,i]`"_.

### Physics Example

In this section, we illustrate the use of eigenvalue solvers in finding stable solutions of the coupled system of oscillators shown below.

![Figure 2 - Coupled oscillators, masses on springs.](masses.png)

If the displacement of the $i$th mass from its equilibrium position is denoted as $x_i$, the force on the mass is given by the tension in the two springs as :

$$F_i = −k(x_i − x_{i−1}) + k(x_{i+1} − x_i) = −k(2x_i − x_{i−1} − x_{i+1})$$

We can assume that there are normal mode solutions, i.e. solutions of the form $x_i = z_i e^{i\omega t}$ in which all masses oscillate with the same frequency $\omega$ but with unknown phasors $z_i$. Then the above equation becomes :

$$F_i = m\ddot{x}_i = −m\omega^2x_i = −k(2x_i − x_{i−1} − x_{i+1})$$

This is one row of a matrix equation describing the entire system :

$$m\omega^2x_i \left(\begin{array}{c} \vdots \\ \\ x_i \\ \\ \vdots \end{array}\right) = 
\left(\begin{array}{ccccccc} & & & \vdots & & & \\ \cdots & 0 & -1 & 2 & -1 & 0 & \cdots \\ & & & \vdots & & & \end{array}\right)
\left(\begin{array}{c} \vdots \\ x_{i-1} \\ x_i \\ x_{i+1} \\ \vdots \end{array}\right)
$$

This example is a typical eigenvalue problem, in that many of the matrix elements are zero, which can greatly simplify the computational challenge and make even large systems solvable. The matrix is symmetric, which means it is suitable for solving with our eigenproblem solving function above, or one of the solvers from `scipy.linalg`.

```{python}
m = np.array([[2, -1,  0,  0,  0,  0,  0],
              [-1, 2, -1,  0,  0,  0,  0],
              [0, -1,  2, -1,  0,  0,  0],
              [0,  0, -1,  2, -1,  0,  0],
              [0,  0,  0, -1,  2, -1,  0],
              [0,  0,  0,  0, -1,  2, -1],
              [0,  0,  0,  0,  0, -1,  2]])

mus, vs = linalg.eig(m)
```

The eigenvalue associated with each mode gives the frequency, while the (complex) eigenvector provides the magnitude and phase of oscillation for each mass. We can plot the displacement of each mass as a function of time for each mode.

```{python}
import numpy as np
import scipy.linalg as linalg
import matplotlib.pyplot as plt

# a function to calculate the (real) displacement from complex phase
def disp(zi, omega, t):
    return np.real(zi * np.exp(1j * omega * t))

# set up some pretty colours for plotting
cm  = plt.cm.viridis
col = [cm(int(x*cm.N/7)) for x in range(7)]

# time period
ts = np.arange(0,40, 0.001)

# loop over eigenmodes
for i in range(7):
    
    print("Mode       : ",i)
    print("Eigenvalue : ", mus[i])

    fig=plt.figure(figsize=(16, 4))

    xs = []
    
    # loop over masses
    for j in range(7):
        
        # get the displacement, and add an offset to separate out each line
        offset = (2*j)-6
        
        # create displacement values from function using eigenvectors and eigenvalues
        xs     = disp(vs.T[i][j], mus[i], ts) + offset
        
        # plot displacement
        plt.plot(ts, xs, color=col[j])
        
        # plot central position to guide the eye
        plt.plot([0, 40], [offset, offset], color=col[j], linestyle='dotted') 

    plt.xlabel("t")
    plt.show()
```
